{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MacroHFT dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "from pandas.plotting import autocorrelation_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_feather(\"data_1/ETHUSDT/df_test.feather\")\n",
    "\n",
    "# Analyze the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Available columns:\", df.columns.values)\n",
    "print(\"Sample data:\\n\", df.head())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_values[missing_values > 0])\n",
    "\n",
    "# Handle missing values if any\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    df = df.fillna(df.median())\n",
    "\n",
    "# Visualize the target variable (price_spread)\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(df['close'].values[:200])\n",
    "plt.title('Close Price (Target Variable) - First 200 Data Points')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Close Price')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Show the distribution of the target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['close'], bins=50)\n",
    "plt.title('Distribution of close')\n",
    "plt.xlabel('close')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Show autocorrelation of the target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "autocorrelation_plot(df['close'].values[:1000])\n",
    "plt.title('Autocorrelation of close')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 10 features (excluding the target itself)\n",
    "selected_features = [\n",
    "    'open',\n",
    "    'close',\n",
    "    'high',\n",
    "    'low',\n",
    "    'volume',\n",
    "]\n",
    "target_column = 'close'\n",
    "\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "# Extract features and target\n",
    "X = df[selected_features].values\n",
    "y = df[target_column].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize the data\n",
    "X_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "# X_scaled = X_scaler.fit_transform(X)\n",
    "# y_scaled = y_scaler.fit_transform(y)\n",
    "\n",
    "# Show the distribution of normalized target\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(df['close'].values[:200])\n",
    "plt.title('Close Price - First 200 Data Points')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Close Price')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "X_test = X[train_size:]\n",
    "y_test = y[train_size:]\n",
    "\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "y_train = y_scaler.fit_transform(y_train)\n",
    "\n",
    "X_test = X_scaler.transform(X_test)\n",
    "y_test = y_scaler.transform(y_test)\n",
    "\n",
    "print(f\"Training data shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Testing data shape: X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "# Create sequences for LSTM\n",
    "def create_sequences(x, y, time_steps=50):\n",
    "    \"\"\"\n",
    "    Create sequences of data suitable for LSTM training\n",
    "    Args:\n",
    "        x: Features array (samples × features)\n",
    "        y: Target array (samples × 1)\n",
    "        time_steps: Number of time steps to look back\n",
    "    \n",
    "    Returns:\n",
    "        X_seq: Sequence of features (samples × time_steps × features)\n",
    "        y_seq: Target values (samples × 1)\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(x) - time_steps):\n",
    "        X_seq.append(x[i:(i + time_steps)])\n",
    "        y_seq.append(y[i + time_steps])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Create sequences\n",
    "time_steps = 50  # Look back 5 time steps\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, time_steps)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, time_steps)\n",
    "\n",
    "print(f\"Training sequences shape: X={X_train_seq.shape}, y={y_train_seq.shape}\")\n",
    "print(f\"Testing sequences shape: X={X_test_seq.shape}, y={y_test_seq.shape}\")\n",
    "\n",
    "# Create PyTorch datasets and loaders\n",
    "batch_size = 64\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_seq)\n",
    "y_train_tensor = torch.FloatTensor(y_train_seq)\n",
    "X_test_tensor = torch.FloatTensor(X_test_seq)\n",
    "y_test_tensor = torch.FloatTensor(y_test_seq)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class VanillaLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1):\n",
    "        super(VanillaLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(2, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Model parameters\n",
    "input_size = len(selected_features)\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "\n",
    "# Create the model\n",
    "model = VanillaLSTM(input_size, hidden_size, output_size)\n",
    "print(model)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with early stopping\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=100, early_stopping_patience=10):\n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Early stopping variables\n",
    "    min_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Move data to device\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                # Move data to device\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                test_loss += loss.item()\n",
    "        \n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {avg_train_loss:.6f}, '\n",
    "              f'Test Loss: {avg_test_loss:.6f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_test_loss < min_val_loss:\n",
    "            min_val_loss = avg_test_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "            break\n",
    "    \n",
    "    # Load the best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Plot the loss curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(test_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return model, train_losses, test_losses\n",
    "\n",
    "# Train the model\n",
    "trained_model, train_losses, test_losses = train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    num_epochs=20,\n",
    "    early_stopping_patience=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate_model(model, X_test_tensor, y_test_tensor, y_scaler):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    X_test_tensor = X_test_tensor.to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        y_pred_scaled = model(X_test_tensor).cpu().numpy()\n",
    "    \n",
    "    # Inverse transform to get actual values\n",
    "    y_pred = y_scaler.inverse_transform(y_pred_scaled)\n",
    "    y_actual = y_scaler.inverse_transform(y_test_tensor.numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_actual, y_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_actual, y_pred)\n",
    "    r2 = r2_score(y_actual, y_pred)\n",
    "    \n",
    "    print(f\"Mean Squared Error (MSE): {mse:.6f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.6f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.6f}\")\n",
    "    print(f\"R² Score: {r2:.6f}\")\n",
    "    \n",
    "    # Plot actual vs predicted values\n",
    "    plt.plot(y_actual[:200], label='Actual Close', alpha=0.7, linewidth=2)\n",
    "    plt.plot(y_pred[:200], label='Predicted Close', linestyle='--', linewidth=1.5)\n",
    "    plt.ylabel('Close Price')\n",
    "    \n",
    "    # Plot the distribution of prediction errors\n",
    "    errors = y_actual - y_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(errors, bins=50)\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Prediction Errors')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred, y_actual, mse, rmse, mae, r2\n",
    "\n",
    "# Evaluate the trained model\n",
    "y_pred, y_actual, mse, rmse, mae, r2 = evaluate_model(trained_model, X_test_tensor, y_test_tensor, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on new data\n",
    "def predict_close_price(model, new_data, X_scaler, y_scaler, time_steps=96):\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Scale the new data\n",
    "    new_data_scaled = X_scaler.transform(new_data)\n",
    "    \n",
    "    # Create sequences for prediction\n",
    "    sequences = []\n",
    "    for i in range(len(new_data_scaled) - time_steps + 1):\n",
    "        sequences.append(new_data_scaled[i:i+time_steps])\n",
    "    \n",
    "    # Convert to tensor\n",
    "    sequences_tensor = torch.FloatTe96or(sequences).to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        predicted_scaled = model(sequences_tensor).cpu().numpy()\n",
    "    \n",
    "    # Inverse transform the predictions\n",
    "    predictions = y_scaler.inverse_transform(predicted_scaled)\n",
    "    \n",
    "    return predictions.flatten()\n",
    "\n",
    "# Example of using the prediction function (assuming you have new data)\n",
    "# This would be applied to your test/validation data\n",
    "new_data = df[selected_features].values[-100:]\n",
    "print(\"New data shape:\", new_data.shape)\n",
    "print(\"New data sample:\\n\", new_data[:5])\n",
    "\n",
    "predictions = predict_close_price(trained_model, new_data, X_scaler, y_scaler)\n",
    "print(\"Predictions shape:\", predictions.shape)\n",
    "print(\"Predictions:\\n\", predictions[:5])\n",
    "\n",
    "# Get the corresponding actual close prices\n",
    "# Remember predictions start at index = time_steps (due to sequence creation)\n",
    "actual_close = df['close'].values[-len(predictions) - time_steps + 1:]\n",
    "\n",
    "# Create time indices for proper alignment\n",
    "time_indices = np.arange(len(actual_close))[-len(predictions):]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot actual prices for the relevant period\n",
    "plt.plot(time_indices, actual_close[-len(predictions):], \n",
    "         label='Actual Close Price', alpha=0.7, linewidth=2)\n",
    "\n",
    "# Plot predicted prices\n",
    "plt.plot(time_indices, predictions, \n",
    "         label='Predicted Close Price', linestyle='--', linewidth=1.5)\n",
    "\n",
    "plt.xlabel('Time Steps (Index)', fontsize=12)\n",
    "plt.ylabel('Close Price', fontsize=12)\n",
    "plt.title('Actual vs Predicted Close Prices', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
