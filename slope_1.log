Namespace(buffer_size=1000000, dataset='ETHUSDT', q_value_memorize_freq=10, batch_size=512, eval_update_freq=100, lr=0.0001, epsilon_start=0.5, epsilon_end=0.1, decay_length=5, update_times=10, gamma=0.99, tau=0.005, transcation_cost=0.0002, back_time_length=1, seed=12345, n_step=1, epoch_number=15, label='label_1', clf='slope', alpha=1.0, device='cuda:0')
low_level_agent->train(): epoch  1
low_level_agent->train(): random_list  []
low_level_agent->train(): epsilon  0.42
low_level_agent->train(): mean_return_rate_train  nan
low_level_agent->train(): mean_final_balance_train  nan
low_level_agent->train(): mean_required_money_train  nan
low_level_agent->train(): mean_reward_sum_train  nan
low_level_agent->train(): return_rate_eval  nan
low_level_agent->train(): return_rate_0  nan
low_level_agent->train(): return_rate_1  nan
low_level_agent->train(): epoch  2
low_level_agent->train(): random_list  []
low_level_agent->train(): epsilon  0.33999999999999997
low_level_agent->train(): mean_return_rate_train  nan
low_level_agent->train(): mean_final_balance_train  nan
low_level_agent->train(): mean_required_money_train  nan
low_level_agent->train(): mean_reward_sum_train  nan
low_level_agent->train(): return_rate_eval  nan
low_level_agent->train(): return_rate_0  nan
low_level_agent->train(): return_rate_1  nan
low_level_agent->train(): epoch  3
low_level_agent->train(): random_list  []
low_level_agent->train(): epsilon  0.26
low_level_agent->train(): mean_return_rate_train  nan
low_level_agent->train(): mean_final_balance_train  nan
low_level_agent->train(): mean_required_money_train  nan
low_level_agent->train(): mean_reward_sum_train  nan
low_level_agent->train(): return_rate_eval  nan
low_level_agent->train(): return_rate_0  nan
low_level_agent->train(): return_rate_1  nan
low_level_agent->train(): epoch  4
low_level_agent->train(): random_list  []
low_level_agent->train(): epsilon  0.17999999999999994
low_level_agent->train(): mean_return_rate_train  nan
low_level_agent->train(): mean_final_balance_train  nan
low_level_agent->train(): mean_required_money_train  nan
low_level_agent->train(): mean_reward_sum_train  nan
low_level_agent->train(): return_rate_eval  nan
low_level_agent->train(): return_rate_0  nan
low_level_agent->train(): return_rate_1  nan
low_level_agent->train(): epoch  5
low_level_agent->train(): random_list  []
low_level_agent->train(): epsilon  0.1
low_level_agent->train(): mean_return_rate_train  nan
low_level_agent->train(): mean_final_balance_train  nan
low_level_agent->train(): mean_required_money_train  nan
low_level_agent->train(): mean_reward_sum_train  nan
low_level_agent->train(): return_rate_eval  nan
low_level_agent->train(): return_rate_0  nan
low_level_agent->train(): return_rate_1  nan
low_level_agent->train(): epoch  6
low_level_agent->train(): random_list  []
low_level_agent->train(): epsilon  0.1
low_level_agent->train(): mean_return_rate_train  nan
low_level_agent->train(): mean_final_balance_train  nan
low_level_agent->train(): mean_required_money_train  nan
low_level_agent->train(): mean_reward_sum_train  nan
low_level_agent->train(): return_rate_eval  nan
low_level_agent->train(): return_rate_0  nan
low_level_agent->train(): return_rate_1  nan
low_level_agent->train(): epoch  7
low_level_agent->train(): random_list  []
low_level_agent->train(): epsilon  0.1
low_level_agent->train(): mean_return_rate_train  nan
low_level_agent->train(): mean_final_balance_train  nan
low_level_agent->train(): mean_required_money_train  nan
low_level_agent->train(): mean_reward_sum_train  nan
low_level_agent->train(): return_rate_eval  nan
low_level_agent->train(): return_rate_0  nan
low_level_agent->train(): return_rate_1  nan
low_level_agent->train(): epoch  8
low_level_agent->train(): random_list  []
low_level_agent->train(): epsilon  0.1
low_level_agent->train(): mean_return_rate_train  nan
low_level_agent->train(): mean_final_balance_train  nan
low_level_agent->train(): mean_required_money_train  nan
low_level_agent->train(): mean_reward_sum_train  nan
low_level_agent->train(): return_rate_eval  nan
low_level_agent->train(): return_rate_0  nan
low_level_agent->train(): return_rate_1  nan
low_level_agent->train(): epoch  9
low_level_agent->train(): random_list  []
low_level_agent->train(): epsilon  0.1
low_level_agent->train(): mean_return_rate_train  nan
low_level_agent->train(): mean_final_balance_train  nan
low_level_agent->train(): mean_required_money_train  nan
low_level_agent->train(): mean_reward_sum_train  nan
low_level_agent->train(): return_rate_eval  nan
low_level_agent->train(): return_rate_0  nan
low_level_agent->train(): return_rate_1  nan
low_level_agent->train(): epoch  10
low_level_agent->train(): random_list  []
low_level_agent->train(): epsilon  0.1
low_level_agent->train(): mean_return_rate_train  nan
low_level_agent->train(): mean_final_balance_train  nan
low_level_agent->train(): mean_required_money_train  nan
low_level_agent->train(): mean_reward_sum_train  nan
low_level_agent->train(): return_rate_eval  nan
low_level_agent->train(): return_rate_0  nan
low_level_agent->train(): return_rate_1  nan
low_level_agent->train(): epoch  11
low_level_agent->train(): random_list  []
low_level_agent->train(): epsilon  0.1
low_level_agent->train(): mean_return_rate_train  nan
low_level_agent->train(): mean_final_balance_train  nan
low_level_agent->train(): mean_required_money_train  nan
low_level_agent->train(): mean_reward_sum_train  nan
low_level_agent->train(): return_rate_eval  nan
low_level_agent->train(): return_rate_0  nan
low_level_agent->train(): return_rate_1  nan
low_level_agent->train(): epoch  12
low_level_agent->train(): random_list  []
low_level_agent->train(): epsilon  0.1
low_level_agent->train(): mean_return_rate_train  nan
low_level_agent->train(): mean_final_balance_train  nan
low_level_agent->train(): mean_required_money_train  nan
low_level_agent->train(): mean_reward_sum_train  nan
low_level_agent->train(): return_rate_eval  nan
low_level_agent->train(): return_rate_0  nan
low_level_agent->train(): return_rate_1  nan
low_level_agent->train(): epoch  13
low_level_agent->train(): random_list  []
low_level_agent->train(): epsilon  0.1
low_level_agent->train(): mean_return_rate_train  nan
low_level_agent->train(): mean_final_balance_train  nan
low_level_agent->train(): mean_required_money_train  nan
low_level_agent->train(): mean_reward_sum_train  nan
low_level_agent->train(): return_rate_eval  nan
low_level_agent->train(): return_rate_0  nan
low_level_agent->train(): return_rate_1  nan
low_level_agent->train(): epoch  14
low_level_agent->train(): random_list  []
low_level_agent->train(): epsilon  0.1
low_level_agent->train(): mean_return_rate_train  nan
low_level_agent->train(): mean_final_balance_train  nan
low_level_agent->train(): mean_required_money_train  nan
low_level_agent->train(): mean_reward_sum_train  nan
low_level_agent->train(): return_rate_eval  nan
low_level_agent->train(): return_rate_0  nan
low_level_agent->train(): return_rate_1  nan
low_level_agent->train(): epoch  15
low_level_agent->train(): random_list  []
low_level_agent->train(): epsilon  0.1
low_level_agent->train(): mean_return_rate_train  nan
low_level_agent->train(): mean_final_balance_train  nan
low_level_agent->train(): mean_required_money_train  nan
low_level_agent->train(): mean_reward_sum_train  nan
low_level_agent->train(): return_rate_eval  nan
low_level_agent->train(): return_rate_0  nan
low_level_agent->train(): return_rate_1  nan
Traceback (most recent call last):
  File "/Users/lehoangsang/MacroHFT/RL/agent/low_level.py", line 520, in <module>
    agent.train()
    ~~~~~~~~~~~^^
  File "/Users/lehoangsang/MacroHFT/RL/agent/low_level.py", line 432, in train
    torch.save(best_model.state_dict(), best_model_path)
               ^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'state_dict'
